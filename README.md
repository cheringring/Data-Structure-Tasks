# 웹 크롤러 및 그래프 시각화 프로젝트


## 📋 목차

1. 프로젝트 개요
2. 주요 기능
3. 시스템 아키텍처
4. 기술 스택
5. 설치 및 실행 방법
6. 사용 방법
7. 최적 설정 가이드
8. 주요 구현 세부사항
9. 문제 해결 및 최적화
10. 성능 분석
11. 확장 가능성
12. 결론
13. 라이센스
14. 기여자


## 프로젝트 개요
이 프로젝트는 C로 구현된 멀티스레드 웹 크롤러와 Python Flask 기반의 웹 인터페이스를 결합하여 웹사이트의 링크 구조를 크롤링하고 시각화하는 시스템입니다. 자료구조와 알고리즘의 실제 응용을 보여주는 교육용 프로젝트로, 그래프 이론, 해시 테이블, 큐 등의 자료구조를 활용합니다.
<br>

웹 크롤러는 지정된 시작 URL에서 출발하여 해당 페이지의 모든 링크를 추출하고, 각 링크를 따라가며 웹사이트의 구조를 탐색합니다. 이 과정에서 발견된 모든 URL과 그들 사이의 연결 관계는 방향 그래프로 표현되며, 최종적으로 Graphviz DOT 형식으로 저장됩니다. 이 그래프는 웹 인터페이스를 통해 시각적으로 표현되어 웹사이트의 구조를 직관적으로 이해할 수 있게 합니다.

<img width="837" alt="image" src="https://github.com/user-attachments/assets/feb1e43f-0bd9-4bd8-b1e5-b3e24e290b9c" />

<br>

## 프로젝트 목표
- 효율적인 멀티스레드 웹 크롤링 구현
- 웹사이트 링크 구조의 그래프 표현 및 시각화
- 자료구조와 알고리즘의 실제 응용 사례 제공
- 다양한 인코딩 및 URL 형식 처리 능력 구현
- 사용자 친화적인 웹 인터페이스 제공

<br><br>

## 학습 목표
- 그래프 자료구조의 실제 응용 이해
- 해시 테이블을 활용한 중복 데이터 처리 방법 학습
- 큐를 활용한 너비 우선 탐색(BFS) 알고리즘 구현
- 멀티스레딩을 통한 병렬 처리 기법 습득
- C와 Python 간의 상호 운용성 이해
<br><br>

## 주요 기능

### 웹 크롤링 기능
- 멀티스레드 크롤링: 여러 스레드를 사용하여 병렬적으로 웹사이트 크롤링
- URL 정규화: 상대 경로를 절대 경로로 변환하고 프래그먼트 제거
- 중복 URL 제거: 해시 테이블을 사용하여 이미 방문한 URL 추적
- 깊이 제한: 최대 크롤링 깊이 설정 가능
- 최대 URL 제한: 크롤링할 최대 URL 수 설정 가능

<br>

### HTML 처리 기능
- HTML 파싱: 효율적인 HTML 문서 파싱
- 링크 추출: <a href="..."> 태그에서 URL 추출
- 인코딩 처리: UTF-8 및 기타 인코딩 형식 지원
- 오류 복원: 손상된 HTML 처리 능력
<br>

### 그래프 생성 및 시각화
-  그래프 구축: 웹사이트의 링크 구조를 방향 그래프로 표현
- DOT 파일 생성: Graphviz 호환 DOT 형식으로 그래프 저장
- 그래프 레이아웃 최적화: 가독성 높은 그래프 레이아웃 설정
- 동적 이미지 생성: 타임스탬프를 활용한 캐시 방지 기능
<br>

### 웹 인터페이스
- 크롤링 설정: URL, 최대 URL 수, 스레드 수 등 설정 가능
- 실시간 로그: 크롤링 진행 상황 실시간 표시
- 결과 시각화: 웹 그래프를 이미지로 표시
- 추천 설정: 다양한 웹사이트에 대한 최적 설정 가이드 제공

<br>

<img width="718" alt="image" src="https://github.com/user-attachments/assets/0d3ee6ac-39c1-438a-80e9-72819b3f27ad" />

<br>

<img width="714" alt="image" src="https://github.com/user-attachments/assets/fcd9da6d-329b-4f53-a44f-1d26fdcfcfc6" />



## 시스템 아키텍처
프로젝트는 크게 두 부분으로 구성됩니다

### 1. C 웹 크롤러 (백엔드)
웹 크롤러는 C 언어로 구현되어 있으며, 다음과 같은 모듈로 구성됩니다:

- main.c: 프로그램의 진입점, 명령줄 인수 처리 및 크롤러 초기화
- crawler.c/h: 크롤링 로직 및 스레드 관리
- url_queue.c/h: 크롤링할 URL을 저장하는 스레드 안전 큐 구현
- url_set.c/h: 방문한 URL을 추적하는 해시 테이블 구현
- web_graph.c/h: 웹 그래프 구현 및 DOT 파일 생성
- html_parser.c/h: HTML 파싱 및 URL 추출
- http_client.c/h: libcurl을 사용한 HTTP 요청 처리
- common.h: 공통 상수 및 유틸리티 함수 정의

### 2. Python Flask 인터페이스 (프론트엔드)
웹 인터페이스는 Python Flask로 구현되어 있으며, 다음과 같은 구성요소를 포함합니다

- visualizer.py: Flask 웹 서버 및 인터페이스 로직
- HTML 템플릿: 사용자 인터페이스 구현
- JavaScript: 비동기 통신 및 UI 업데이트
- CSS: 스타일링 및 레이아웃

### 데이터 흐름
1. 사용자가 웹 인터페이스에서 크롤링 매개변수 설정
2. Flask 서버가 C 크롤러를 실행하고 매개변수 전달
3. 크롤러가 웹사이트 크롤링 및 그래프 생성
4. DOT 파일이 생성되고 Graphviz로 처리
5. 결과 이미지가 웹 인터페이스에 표시

<img width="447" alt="image" src="https://github.com/user-attachments/assets/5bfc771d-669b-46ba-b5f1-e68fd9dcfd30" />

<br><br>

## 기술 스택

### 백엔드 (C 웹 크롤러)
- C 언어: 핵심 크롤링 엔진 구현
- libcurl: HTTP 요청 처리
- pthread: 멀티스레딩 구현
- POSIX API: 시스템 호출 및 파일 처리
<br>

### 프론트엔드 (Python 웹 인터페이스)
- Python 3: 서버 사이드 로직
- Flask: 웹 프레임워크
- HTML/CSS/JavaScript: 사용자 인터페이스
- AJAX: 비동기 통신
<br>

### 도구 및 라이브러리
- Graphviz: 그래프 시각화
- Make: 빌드 자동화
- GCC: C 코드 컴파일
- JSON: 데이터 교환 형식
<br>

## 설치 및 실행 방법

### 사전 요구사항

- C 언어: C 컴파일러 (GCC 7.0 이상)
- Python 3: Python 3.6 이상
- Graphviz: 그래프 시각화 도구 (2.40 이상)
- Make: 빌드 자동화 도구
- libcurl: HTTP 클라이언트 라이브러리
  
<br><br>
### 의존성 설치

- Ubuntu/Debian

```
# 시스템 패키지 설치
sudo apt-get update
sudo apt-get install -y build-essential libcurl4-openssl-dev python3 python3-pip graphviz

# Python 패키지 설치
pip3 install flask

```

<br>

- macOS

```
# Homebrew를 사용한 패키지 설치
brew update
brew install gcc make curl python3 graphviz

# Python 패키지 설치
pip3 install flask
```

<br>


### 웹 크롤러 컴파일

```
# WebCrawler 디렉토리로 이동
cd WebCrawler

# 이전 빌드 정리 및 새로 컴파일
make clean && make
```

<br>

### 웹 인터페이스 실행

```
# Interface 디렉토리로 이동
cd Interface

# Flask 서버 실행
python3 visualizer.py
```

<br>

### 웹 브라우저에서 http://127.0.0.1:5000/ 접속

> WebCrawler 폴더 ->  makefile을 통해 생성된 make를 사용해 컴파일해서 오브젝트 파일 생성
<img width="720" alt="image" src="https://github.com/user-attachments/assets/c0897d41-945e-413a-b898-6cd16272efc0" />


<br><br>


> Interface 폴더 -> python3 파일이름 실행 -> 5000 port run 화면
<img width="714" alt="image" src="https://github.com/user-attachments/assets/d03b81a3-5f6c-4f9b-ab90-de289870a2a8" />


<br>
<br>

### 사용 방법

1. 크롤링 설정
웹 인터페이스에서 크롤링할 URL 입력 (예: https://news.ycombinator.com)
최대 URL 수 설정 (추천: 20-30개)
스레드 수 설정 (추천: 4개)
"크롤링 시작" 버튼 클릭

<img width="238" alt="image" src="https://github.com/user-attachments/assets/93b1ab89-373f-44a4-badf-d4fd41e257d8" />

<br>

2. 크롤링 모니터링
실시간 로그를 통해 크롤링 진행 상황 확인
발견된 URL 및 처리 상태 확인
오류 및 경고 메시지 확인

<img width="714" alt="image" src="https://github.com/user-attachments/assets/4dd2fdbf-3b98-4688-afc3-5540ddf3e85a" />


<br>
4. 결과 시각화
크롤링 완료 후 "결과 시각화" 버튼 클릭
생성된 웹 그래프 이미지 확인
필요에 따라 이미지 저장 또는 공유

<img width="731" alt="image" src="https://github.com/user-attachments/assets/8c45ded5-9237-493b-8c5b-9dd21641c17b" />



<br>

5. 고급 사용법
다양한 웹사이트에서 테스트하여 최적의 설정 찾기
크롤링 결과 비교 및 분석
DOT 파일 직접 수정하여 그래프 사용자 정의
<br>


## 최적 설정 가이드

### 추천 설정 표

<img width="716" alt="image" src="https://github.com/user-attachments/assets/44b757f5-9db8-4cae-b323-c525cbc5dbaa" />

### 설정 최적화 팁

- URL 수 조정:
너무 적으면 의미 있는 그래프 구조를 보기 어려움
너무 많으면 그래프가 복잡해져 가독성 저하
일반적으로 20-30개가 최적

- 스레드 수 조정:
너무 적으면 크롤링 속도가 느림
너무 많으면 네트워크 부하 및 차단 위험
일반적으로 4-8개가 최적

### 크롤링 대상 선택

링크가 많은 웹사이트 선택 (뉴스, 포털 등)
정적 콘텐츠 위주의 사이트가 분석하기 좋음
JavaScript 의존도가 낮은 사이트 선택


## 주요 구현 세부사항

### 웹 크롤러 (C)

#### 자료구조

- URL 큐: 크롤링할 URL을 저장하는 큐 자료구조가 url_queue.c/h에 구현되어 있습니다. 멀티스레드 환경에서 안전하게 작동하도록 뮤텍스를 사용합니다.
  
- URL 집합: 이미 방문한 URL을 추적하는 해시 테이블이 url_set.c/h에 구현되어 있습니다. URL 중복을 효율적으로 감지합니다.
  
- 웹 그래프: 웹사이트의 링크 구조를 표현하는 방향 그래프가 web_graph.c/h에 구현되어 있습니다. 인접 리스트 방식으로 구현되어 메모리 효율성을 높였습니다.
  
크롤러: 전체 크롤링 프로세스를 관리하는 구조체가 crawler.c/h에 구현되어 있습니다. URL 큐, URL 집합, 웹 그래프를 통합 관리합니다.

<br>

#### 주요 알고리즘
- BFS 크롤링 알고리즘: crawler.c의 crawler_thread 함수에 구현되어 있습니다. 너비 우선 탐색(BFS) 방식으로 웹 사이트를 크롤링합니다.
- URL 정규화 알고리즘: html_parser.c의 normalize_url 함수에 구현되어 있습니다. 상대 경로를 절대 경로로 변환하고 URL을 정규화합니다.
- UTF-8 유효성 검사: http_client.c의 is_valid_utf8 함수에 구현되어 있습니다. 올바른 UTF-8 시퀀스인지 검사하고 처리합니다.
<br>
  
### 웹 인터페이스 (Python)

#### 주요 기능
- 크롤러 실행 함수: visualizer.py의 run_crawler 함수에서 C 크롤러를 실행하고 출력을 모니터링합니다.
  
- 실시간 상태 업데이트: JavaScript를 통해 /status 엔드포인트로 주기적으로 요청을 보내 크롤링 상태를 업데이트합니다.

- 그래프 시각화 처리: visualizer.py의 generate_graph_image 함수에서 Graphviz를 사용하여 DOT 파일을 PNG 이미지로 변환합니다.

<br><br>

##  문제 해결 및 최적화
### 해결된 주요 문제

#### 1. 인코딩 문제
- 문제: 일부 웹사이트에서 UTF-8이 아닌 인코딩 사용으로 파싱 오류 발생
- 해결:
http_client.c에 UTF-8 유효성 검사 함수 추가
Latin-1 등 대체 인코딩 처리 로직 구현
손상된 문자 필터링 메커니즘 추가
<br>

#### 2.  최대 정점 수 제한
- 문제: 초기 구현에서 최대 정점 수가 8로 제한되어 대규모 크롤링 불가
- 해결:
common.h의 MAX_VERTICES 상수를 100으로 증가
main.c에서 크롤러 초기화 시 MAX_VERTICES 사용
동적 메모리 할당 개선으로 메모리 사용 최적화
<br>

#### 3. 그래프 시각화 개선
- 문제: 복잡한 그래프에서 가독성 저하 문제
- 해결:
web_graph.c에서 DOT 파일 생성 시 노드 크기 및 간격 최적화
간선 스타일 및 화살표 크기 조정
그래프 레이아웃 알고리즘 개선
<br>

#### 4. 동적 이미지 생성
- 문제: 브라우저 캐싱으로 인해 그래프 이미지 업데이트 안 됨
- 해결:
visualizer.py에서 타임스탬프와 랜덤 접미사를 사용한 고유 파일명 생성
이전 이미지 파일 자동 정리 (최신 10개만 유지)


### 성능 최적화

#### 1. 메모리 사용량 최적화
URL 문자열 중복 저장 방지를 위한 해시 테이블 구현
크롤링 완료된 URL 데이터 즉시 해제
버퍼 크기 최적화

#### 2. 크롤링 속도 개선
스레드 풀 크기 최적화 (기본값: 4)
네트워크 타임아웃 설정 조정
HTML 파싱 알고리즘 개선


#### 3. UI 응답성 향상
비동기 AJAX 요청을 통한 실시간 상태 업데이트
점진적 UI 업데이트
로그 표시 최적화

<br>
<br>

## 성능 분석

### 병렬화 효율성

#### 스레드 수 증가에 따른 성능 향상:

1 → 2 스레드: 약 80% 성능 향상
2 → 4 스레드: 약 50% 성능 향상
4 → 8 스레드: 약 20% 성능 향상
8개 이상의 스레드에서는 네트워크 I/O 병목으로 인해 추가 성능 향상이 제한적입니다.
<br>


### 메모리 사용량

기본 메모리 사용량: ~3MB
URL당 평균 메모리 증가: ~0.2MB
100개 URL 크롤링 시 최대 메모리: ~25MB

> matplotlib으로 데이터 시각화 
<img width="791" alt="image" src="https://github.com/user-attachments/assets/3f72d913-8f68-4e1a-ab46-3a447a218def" />


위 데이터를 통해 URL 수가 증가함에 따라 실행 시간과 메모리 사용량이 선형적으로 증가하는 것을 확인할 수 있습니다.
특히 URL 처리 효율(URL/초)은 URL 수가 많아질수록 오히려 소폭 증가하는 경향을 보여, 
멀티스레딩의 효과가 대규모 크롤링에서 더 효율적으로 나타남을 알 수 있습니다.

<br><br>

## 확장 가능성
이 프로젝트는 다음과 같은 방향으로 확장할 수 있습니다

### 기능 확장
#### 1. 고급 필터링
특정 도메인이나 경로만 크롤링하는 옵션
정규식 기반 URL 필터링
콘텐츠 타입 기반 필터링
<br>

#### 2. 콘텐츠 분석
웹페이지 내용 분석 및 키워드 추출
텍스트 마이닝 및 감성 분석
이미지 및 미디어 파일 추출
<br>

#### 3. 분산 크롤링
여러 머신에서 동시에 크롤링하는 기능
작업 분배 및 결과 병합 메커니즘
로드 밸런싱 및 장애 복구
<br>
<br>

### 기술적 개선

#### 1. 데이터 저장
크롤링 결과를 데이터베이스에 저장
증분 크롤링 및 히스토리 추적
결과 내보내기 및 가져오기 기능

#### 2. 고급 시각화
대화형 그래프 시각화 (D3.js 등 사용)
클러스터링 및 커뮤니티 감지
시간에 따른 웹사이트 구조 변화 추적

#### 3. 보안 및 규정 준수
robots.txt 준수 기능 강화
사용자 에이전트 설정 및 요청 제한
개인정보 보호 및 데이터 익명화

<br><br>

## 결론
이 웹 크롤러 및 그래프 시각화 프로젝트는 자료구조와 알고리즘의 실제를 응용 해보았습니다. C 언어로 구현된 효율적인 멀티스레드 크롤링 엔진과 Python Flask 기반의 사용자 친화적인 웹 인터페이스를 결합하여, 웹사이트의 링크 구조를 탐색하고 시각화하는 기능을 제공합니다.
<br><br>

## 주요 성과:

- 최대 100개의 URL을 효율적으로 크롤링하는 능력
- 다양한 인코딩 형식 처리 및 오류 복원 메커니즘
- 가독성 높은 그래프 시각화 구현
- 사용자 친화적인 웹 인터페이스 제공
  
이 프로젝트는 그래프 이론, 해시 테이블, 큐, 멀티스레딩 등의 개념을 실제로 구현하고 시각화함으로써 이론적 지식을 실무에 적용하는 방법을 학습할 수 있었습니다. 또한, 웹 크롤링, 데이터 처리, 시각화 등의 실용적인 기술을 습득할 수 있었습니다.

<img width="542" alt="image" src="https://github.com/user-attachments/assets/a8db2198-a2de-47a2-851f-ad04864e1635" />

<br><br>

## 라이센스
이 프로젝트는 MIT 라이센스 하에 배포됩니다.
<br><br>

## 기여자
- 학번: [22221617]
- 이름: [권체은]
- 이메일: [gksmfthsu000@gmail.com]

<br>
© 2025 웹 크롤러 및 그래프 시각화 프로젝트
