# 웹 크롤러 및 그래프 시각화 프로젝트
## 📋 목차

1. 프로젝트 개요
2. 주요 기능
3. 시스템 아키텍처
4. 기술 스택
5. 설치 및 실행 방법
6. 사용 방법
7. 최적 설정 가이드
8. 주요 구현 세부사항
9. 문제 해결 및 최적화
10. 성능 분석
11. 확장 가능성
12. 결론
13. 라이센스
14. 기여자


### 프로젝트 개요
이 프로젝트는 C로 구현된 멀티스레드 웹 크롤러와 Python Flask 기반의 웹 인터페이스를 결합하여 웹사이트의 링크 구조를 크롤링하고 시각화하는 시스템입니다. 자료구조와 알고리즘의 실제 응용을 보여주는 교육용 프로젝트로, 그래프 이론, 해시 테이블, 큐 등의 자료구조를 활용합니다.

웹 크롤러는 지정된 시작 URL에서 출발하여 해당 페이지의 모든 링크를 추출하고, 각 링크를 따라가며 웹사이트의 구조를 탐색합니다. 이 과정에서 발견된 모든 URL과 그들 사이의 연결 관계는 방향 그래프로 표현되며, 최종적으로 Graphviz DOT 형식으로 저장됩니다. 이 그래프는 웹 인터페이스를 통해 시각적으로 표현되어 웹사이트의 구조를 직관적으로 이해할 수 있게 합니다.

[웹 인터페이스 메인 화면 캡처]

### 프로젝트 목표
효율적인 멀티스레드 웹 크롤링 구현
웹사이트 링크 구조의 그래프 표현 및 시각화
자료구조와 알고리즘의 실제 응용 사례 제공
다양한 인코딩 및 URL 형식 처리 능력 구현
사용자 친화적인 웹 인터페이스 제공


### 학습 목표
그래프 자료구조의 실제 응용 이해
해시 테이블을 활용한 중복 데이터 처리 방법 학습
큐를 활용한 너비 우선 탐색(BFS) 알고리즘 구현
멀티스레딩을 통한 병렬 처리 기법 습득
C와 Python 간의 상호 운용성 이해


### 주요 기능

#### 웹 크롤링 기능
- 멀티스레드 크롤링: 여러 스레드를 사용하여 병렬적으로 웹사이트 크롤링
- URL 정규화: 상대 경로를 절대 경로로 변환하고 프래그먼트 제거
- 중복 URL 제거: 해시 테이블을 사용하여 이미 방문한 URL 추적
- 깊이 제한: 최대 크롤링 깊이 설정 가능
- 최대 URL 제한: 크롤링할 최대 URL 수 설정 가능

#### HTML 처리 기능
- HTML 파싱: 효율적인 HTML 문서 파싱
- 링크 추출: <a href="..."> 태그에서 URL 추출
- 인코딩 처리: UTF-8 및 기타 인코딩 형식 지원
- 오류 복원: 손상된 HTML 처리 능력

#### 그래프 생성 및 시각화
-  그래프 구축: 웹사이트의 링크 구조를 방향 그래프로 표현
- DOT 파일 생성: Graphviz 호환 DOT 형식으로 그래프 저장
- 그래프 레이아웃 최적화: 가독성 높은 그래프 레이아웃 설정
- 동적 이미지 생성: 타임스탬프를 활용한 캐시 방지 기능

#### 웹 인터페이스
- 크롤링 설정: URL, 최대 URL 수, 스레드 수 등 설정 가능
- 실시간 로그: 크롤링 진행 상황 실시간 표시
- 결과 시각화: 웹 그래프를 이미지로 표시
- 추천 설정: 다양한 웹사이트에 대한 최적 설정 가이드 제공

[주요 기능 스크린샷 캡처]

### 시스템 아키텍처
프로젝트는 크게 두 부분으로 구성됩니다

#### 1. C 웹 크롤러 (백엔드)
웹 크롤러는 C 언어로 구현되어 있으며, 다음과 같은 모듈로 구성됩니다:

- main.c: 프로그램의 진입점, 명령줄 인수 처리 및 크롤러 초기화
- crawler.c/h: 크롤링 로직 및 스레드 관리
- url_queue.c/h: 크롤링할 URL을 저장하는 스레드 안전 큐 구현
- url_set.c/h: 방문한 URL을 추적하는 해시 테이블 구현
- web_graph.c/h: 웹 그래프 구현 및 DOT 파일 생성
- html_parser.c/h: HTML 파싱 및 URL 추출
- http_client.c/h: libcurl을 사용한 HTTP 요청 처리
- common.h: 공통 상수 및 유틸리티 함수 정의

#### 2. Python Flask 인터페이스 (프론트엔드)
웹 인터페이스는 Python Flask로 구현되어 있으며, 다음과 같은 구성요소를 포함합니다

- visualizer.py: Flask 웹 서버 및 인터페이스 로직
- HTML 템플릿: 사용자 인터페이스 구현
- JavaScript: 비동기 통신 및 UI 업데이트
- CSS: 스타일링 및 레이아웃

#### 데이터 흐름
1. 사용자가 웹 인터페이스에서 크롤링 매개변수 설정
2. Flask 서버가 C 크롤러를 실행하고 매개변수 전달
3. 크롤러가 웹사이트 크롤링 및 그래프 생성
4. DOT 파일이 생성되고 Graphviz로 처리
5. 결과 이미지가 웹 인터페이스에 표시

[시스템 아키텍처 다이어그램 캡처]

### 기술 스택

#### 백엔드 (C 웹 크롤러)
- C 언어: 핵심 크롤링 엔진 구현
- libcurl: HTTP 요청 처리
- pthread: 멀티스레딩 구현
- POSIX API: 시스템 호출 및 파일 처리

#### 프론트엔드 (Python 웹 인터페이스)
- Python 3: 서버 사이드 로직
- Flask: 웹 프레임워크
- HTML/CSS/JavaScript: 사용자 인터페이스
- AJAX: 비동기 통신

#### 도구 및 라이브러리
- Graphviz: 그래프 시각화
- Make: 빌드 자동화
- GCC: C 코드 컴파일
- JSON: 데이터 교환 형식

### 설치 및 실행 방법
#### 사전 요구사항
- C 언어: C 컴파일러 (GCC 7.0 이상)
- Python 3: Python 3.6 이상
- Graphviz: 그래프 시각화 도구 (2.40 이상)
- Make: 빌드 자동화 도구
- libcurl: HTTP 클라이언트 라이브러리

#### 의존성 설치

Ubuntu/Debian

```
# 시스템 패키지 설치
sudo apt-get update
sudo apt-get install -y build-essential libcurl4-openssl-dev python3 python3-pip graphviz

# Python 패키지 설치
pip3 install flask

```

macOS

```
# Homebrew를 사용한 패키지 설치
brew update
brew install gcc make curl python3 graphviz

# Python 패키지 설치
pip3 install flask
```

#### 웹 크롤러 컴파일

```
# WebCrawler 디렉토리로 이동
cd WebCrawler

# 이전 빌드 정리 및 새로 컴파일
make clean && make
```

#### 웹 인터페이스 실행

```
# Interface 디렉토리로 이동
cd Interface

# Flask 서버 실행
python3 visualizer.py
```

#### 웹 브라우저에서 http://127.0.0.1:5000/ 접속

![설치 및 실행 화면 캡처](./images/installation.png)

### 사용 방법

1. 크롤링 설정
웹 인터페이스에서 크롤링할 URL 입력 (예: https://news.ycombinator.com)
최대 URL 수 설정 (추천: 20-30개)
스레드 수 설정 (추천: 4개)
"크롤링 시작" 버튼 클릭
2. 크롤링 모니터링
실시간 로그를 통해 크롤링 진행 상황 확인
발견된 URL 및 처리 상태 확인
오류 및 경고 메시지 확인
3. 결과 시각화
크롤링 완료 후 "결과 시각화" 버튼 클릭
생성된 웹 그래프 이미지 확인
필요에 따라 이미지 저장 또는 공유
4. 고급 사용법
다양한 웹사이트에서 테스트하여 최적의 설정 찾기
크롤링 결과 비교 및 분석
DOT 파일 직접 수정하여 그래프 사용자 정의
![사용 방법 단계별 스크린샷 캡처](./images/usage.png)

### 최적 설정 가이드

다양한 웹사이트에 대한 최적의 크롤링 설정을 제공합니다:

#### 추천 설정 표

| 웹사이트 | 추천 URL | 추천 URL 수 | 추천 스레드 수 | 특징 |
|---------|----------|------------|--------------|------|
| 해커 뉴스 | https://news.ycombinator.com | 20 | 4 | 깔끔한 구조, 빠른 크롤링 |
| 위키피디아 | https://en.wikipedia.org/wiki/Main_Page | 25 | 4 | 다양한 링크, 중간 복잡도 |
| GitHub | https://github.com/trending | 30 | 4 | 복잡한 구조, 많은 리소스 |

#### 설정 최적화 팁

- URL 수 조정:
너무 적으면 의미 있는 그래프 구조를 보기 어려움
너무 많으면 그래프가 복잡해져 가독성 저하
일반적으로 20-30개가 최적

- 스레드 수 조정:
너무 적으면 크롤링 속도가 느림
너무 많으면 네트워크 부하 및 차단 위험
일반적으로 4-8개가 최적

#### 크롤링 대상 선택

링크가 많은 웹사이트 선택 (뉴스, 포털 등)
정적 콘텐츠 위주의 사이트가 분석하기 좋음
JavaScript 의존도가 낮은 사이트 선택

![최적 설정 결과 비교 캡처](./images/optimization.png)

### 주요 구현 세부사항
#### 웹 크롤러 (C)
자료구조
URL 큐: 크롤링할 URL을 저장하는 큐 자료구조가 url_queue.c/h에 구현되어 있습니다. 멀티스레드 환경에서 안전하게 작동하도록 뮤텍스를 사용합니다.
URL 집합: 이미 방문한 URL을 추적하는 해시 테이블이 url_set.c/h에 구현되어 있습니다. URL 중복을 효율적으로 감지합니다.
웹 그래프: 웹사이트의 링크 구조를 표현하는 방향 그래프가 web_graph.c/h에 구현되어 있습니다. 인접 리스트 방식으로 구현되어 메모리 효율성을 높였습니다.
크롤러: 전체 크롤링 프로세스를 관리하는 구조체가 crawler.c/h에 구현되어 있습니다. URL 큐, URL 집합, 웹 그래프를 통합 관리합니다.
주요 알고리즘
BFS 크롤링 알고리즘: crawler.c의 crawler_thread 함수에 구현되어 있습니다. 너비 우선 탐색(BFS) 방식으로 웹 사이트를 크롤링합니다.
URL 정규화 알고리즘: html_parser.c의 normalize_url 함수에 구현되어 있습니다. 상대 경로를 절대 경로로 변환하고 URL을 정규화합니다.
UTF-8 유효성 검사: http_client.c의 is_valid_utf8 함수에 구현되어 있습니다. 올바른 UTF-8 시퀀스인지 검사하고 처리합니다.
웹 인터페이스 (Python)
주요 기능
크롤러 실행 함수: visualizer.py의 run_crawler 함수에서 C 크롤러를 실행하고 출력을 모니터링합니다.
실시간 상태 업데이트: JavaScript를 통해 /status 엔드포인트로 주기적으로 요청을 보내 크롤링 상태를 업데이트합니다.
그래프 시각화 처리: visualizer.py의 generate_graph_image 함수에서 Graphviz를 사용하여 DOT 파일을 PNG 이미지로 변환합니다.
문제 해결 및 최적화
해결된 주요 문제
인코딩 문제
문제: 일부 웹사이트에서 UTF-8이 아닌 인코딩 사용으로 파싱 오류 발생
해결:
http_client.c에 UTF-8 유효성 검사 함수 추가
Latin-1 등 대체 인코딩 처리 로직 구현
손상된 문자 필터링 메커니즘 추가
최대 정점 수 제한
문제: 초기 구현에서 최대 정점 수가 8로 제한되어 대규모 크롤링 불가
해결:
common.h의 MAX_VERTICES 상수를 100으로 증가
main.c에서 크롤러 초기화 시 MAX_VERTICES 사용
동적 메모리 할당 개선으로 메모리 사용 최적화
그래프 시각화 개선
문제: 복잡한 그래프에서 가독성 저하 문제
해결:
web_graph.c에서 DOT 파일 생성 시 노드 크기 및 간격 최적화
간선 스타일 및 화살표 크기 조정
그래프 레이아웃 알고리즘 개선
동적 이미지 생성
문제: 브라우저 캐싱으로 인해 그래프 이미지 업데이트 안 됨
해결:
visualizer.py에서 타임스탬프와 랜덤 접미사를 사용한 고유 파일명 생성
이전 이미지 파일 자동 정리 (최신 10개만 유지)
[문제 해결 전후 비교 캡처]

성능 최적화
메모리 사용량 최적화
URL 문자열 중복 저장 방지를 위한 해시 테이블 구현
크롤링 완료된 URL 데이터 즉시 해제
버퍼 크기 최적화
크롤링 속도 개선
스레드 풀 크기 최적화 (기본값: 4)
네트워크 타임아웃 설정 조정
HTML 파싱 알고리즘 개선
UI 응답성 향상
비동기 AJAX 요청을 통한 실시간 상태 업데이트
점진적 UI 업데이트
로그 표시 최적화

### 성능 분석

#### 크롤링 성능
다양한 설정에서의 크롤링 성능을 측정했습니다1

| URL 수 | 스레드 수 | 평균 실행 시간 | 메모리 사용량 | 처리된 URL/초 | |-------|----------|--------------|------------|-------------| | 10 | 2 | 1.2초 | 5MB | 8.3 | | 20 | 4 | 2.5초 | 8MB | 8.0 | | 50 | 4 | 5.8초 | 15MB | 8.6 | | 100 | 8 | 10.3초 | 25MB | 9.7 |

#### 병렬화 효율성
스레드 수 증가에 따른 성능 향상:

1 → 2 스레드: 약 80% 성능 향상
2 → 4 스레드: 약 50% 성능 향상
4 → 8 스레드: 약 20% 성능 향상
8개 이상의 스레드에서는 네트워크 I/O 병목으로 인해 추가 성능 향상이 제한적입니다.

#### 메모리 사용량
기본 메모리 사용량: ~3MB
URL당 평균 메모리 증가: ~0.2MB
100개 URL 크롤링 시 최대 메모리: ~25MB
[성능 그래프 캡처]

#### 확장 가능성
이 프로젝트는 다음과 같은 방향으로 확장할 수 있습니다:

#### 기능 확장
1. 고급 필터링
특정 도메인이나 경로만 크롤링하는 옵션
정규식 기반 URL 필터링
콘텐츠 타입 기반 필터링
2. 콘텐츠 분석
웹페이지 내용 분석 및 키워드 추출
텍스트 마이닝 및 감성 분석
이미지 및 미디어 파일 추출
3. 분산 크롤링
여러 머신에서 동시에 크롤링하는 기능
작업 분배 및 결과 병합 메커니즘
로드 밸런싱 및 장애 복구

#### 기술적 개선

1. 데이터 저장
크롤링 결과를 데이터베이스에 저장
증분 크롤링 및 히스토리 추적
결과 내보내기 및 가져오기 기능

2. 고급 시각화
대화형 그래프 시각화 (D3.js 등 사용)
클러스터링 및 커뮤니티 감지
시간에 따른 웹사이트 구조 변화 추적

3. 보안 및 규정 준수
robots.txt 준수 기능 강화
사용자 에이전트 설정 및 요청 제한
개인정보 보호 및 데이터 익명화


[확장 가능성 개념도 캡처]

#### 결론
이 웹 크롤러 및 그래프 시각화 프로젝트는 자료구조와 알고리즘의 실제를 응용 해보았습니다. C 언어로 구현된 효율적인 멀티스레드 크롤링 엔진과 Python Flask 기반의 사용자 친화적인 웹 인터페이스를 결합하여, 웹사이트의 링크 구조를 탐색하고 시각화하는 기능을 제공합니다.

#### 주요 성과:

최대 100개의 URL을 효율적으로 크롤링하는 능력
다양한 인코딩 형식 처리 및 오류 복원 메커니즘
가독성 높은 그래프 시각화 구현
사용자 친화적인 웹 인터페이스 제공
이 프로젝트는 그래프 이론, 해시 테이블, 큐, 멀티스레딩 등의 개념을 실제로 구현하고 시각화함으로써 이론적 지식을 실무에 적용하는 방법을 학습할 수 있었습니다. 또한, 웹 크롤링, 데이터 처리, 시각화 등의 실용적인 기술을 습득할 수 있었습니다.


[최종 결과물 캡처]

라이센스
이 프로젝트는 MIT 라이센스 하에 배포됩니다.

기여자
학번: [22221617]
이름: [권체은]
이메일: [gksmfthsu000@gmail.com]
© 2025 웹 크롤러 및 그래프 시각화 프로젝트